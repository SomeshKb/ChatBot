{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "# For making a precision, recall report and confusion matrix on the classes\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "anger_training_set = []\n",
    "fear_training_set = []\n",
    "sadness_training_set = []\n",
    "joy_training_set = []\n",
    "\n",
    "anger_test_set = []\n",
    "fear_test_set = []\n",
    "sadness_test_set = []\n",
    "joy_test_set = []\n",
    "stemmer = LancasterStemmer()\n",
    "all_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['how', 'the', 'fuk', 'who', 'the', 'heck', 'moved', 'my', 'fridge', 'should', 'i', 'knock', 'the', 'landlord', 'door', 'angry', 'mad'], 'anger')\n",
      "760\n",
      "995\n",
      "673\n",
      "714\n",
      "4689\n",
      "0\n",
      "['anger', 'fear', 'sadness', 'joy']\n",
      "(3751, 7587)\n",
      "(3751, 4)\n",
      "(938, 7587)\n",
      "(938, 4)\n",
      " Shape of X is  (7587, 3751)\n",
      " Shape of Y is  (4, 3751)\n",
      " Shape of m is  3751\n",
      " Shape of W1 is  (100, 7587)\n",
      " Shape of W2 is  (4, 100)\n",
      "################### TRAIN MODEL STATISTICS ######################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.14      0.30      0.19       463\n",
      "          1       0.37      0.31      0.34      1289\n",
      "          2       0.42      0.25      0.32      1372\n",
      "          3       0.10      0.15      0.12       627\n",
      "\n",
      "avg / total       0.32      0.26      0.28      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.64      0.71      1209\n",
      "          1       0.88      0.60      0.72      1571\n",
      "          2       0.37      0.89      0.53       350\n",
      "          3       0.59      0.84      0.69       621\n",
      "\n",
      "avg / total       0.76      0.68      0.69      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.93      0.82       758\n",
      "          1       0.83      0.87      0.85      1011\n",
      "          2       0.88      0.65      0.75      1132\n",
      "          3       0.82      0.86      0.83       850\n",
      "\n",
      "avg / total       0.82      0.81      0.81      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.68      0.78      1294\n",
      "          1       0.88      0.81      0.84      1160\n",
      "          2       0.69      0.98      0.81       584\n",
      "          3       0.77      0.96      0.85       713\n",
      "\n",
      "avg / total       0.84      0.82      0.82      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      1.00      0.79       618\n",
      "          1       0.75      0.99      0.85       808\n",
      "          2       0.97      0.56      0.71      1458\n",
      "          3       0.84      0.87      0.86       867\n",
      "\n",
      "avg / total       0.84      0.79      0.79      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.57      0.72      1647\n",
      "          1       0.88      0.83      0.85      1133\n",
      "          2       0.40      0.99      0.57       338\n",
      "          3       0.71      1.00      0.83       633\n",
      "\n",
      "avg / total       0.85      0.76      0.76      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      1.00      0.79       619\n",
      "          1       0.75      0.99      0.85       808\n",
      "          2       0.94      0.64      0.76      1222\n",
      "          3       0.93      0.76      0.84      1102\n",
      "\n",
      "avg / total       0.85      0.81      0.81      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.70      0.81      1324\n",
      "          1       0.92      0.88      0.90      1112\n",
      "          2       0.71      0.98      0.82       603\n",
      "          3       0.79      0.99      0.88       712\n",
      "\n",
      "avg / total       0.88      0.86      0.85      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.95      0.90       863\n",
      "          1       0.90      0.96      0.93       997\n",
      "          2       0.87      0.91      0.89       798\n",
      "          3       0.97      0.79      0.87      1093\n",
      "\n",
      "avg / total       0.90      0.90      0.90      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.85      0.90      1076\n",
      "          1       0.93      0.93      0.93      1075\n",
      "          2       0.85      0.96      0.90       739\n",
      "          3       0.92      0.95      0.94       861\n",
      "\n",
      "avg / total       0.92      0.92      0.92      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.91      0.92       979\n",
      "          1       0.94      0.95      0.94      1053\n",
      "          2       0.88      0.95      0.92       773\n",
      "          3       0.95      0.90      0.92       946\n",
      "\n",
      "avg / total       0.93      0.93      0.93      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.89      0.92      1022\n",
      "          1       0.94      0.95      0.95      1065\n",
      "          2       0.88      0.96      0.92       767\n",
      "          3       0.95      0.94      0.94       897\n",
      "\n",
      "avg / total       0.93      0.93      0.93      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.91      0.93       997\n",
      "          1       0.95      0.95      0.95      1068\n",
      "          2       0.89      0.96      0.93       775\n",
      "          3       0.96      0.94      0.95       911\n",
      "\n",
      "avg / total       0.94      0.94      0.94      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.92      0.94      1001\n",
      "          1       0.96      0.96      0.96      1065\n",
      "          2       0.90      0.96      0.93       783\n",
      "          3       0.96      0.95      0.95       902\n",
      "\n",
      "avg / total       0.95      0.95      0.95      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.92      0.94       997\n",
      "          1       0.96      0.96      0.96      1069\n",
      "          2       0.91      0.97      0.94       787\n",
      "          3       0.96      0.95      0.96       898\n",
      "\n",
      "avg / total       0.95      0.95      0.95      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.92      0.94      1000\n",
      "          1       0.96      0.97      0.96      1064\n",
      "          2       0.91      0.96      0.94       791\n",
      "          3       0.96      0.96      0.96       896\n",
      "\n",
      "avg / total       0.95      0.95      0.95      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.93      0.95       996\n",
      "          1       0.96      0.97      0.97      1066\n",
      "          2       0.92      0.96      0.94       796\n",
      "          3       0.96      0.96      0.96       893\n",
      "\n",
      "avg / total       0.96      0.96      0.96      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.93      0.95       992\n",
      "          1       0.97      0.97      0.97      1066\n",
      "          2       0.92      0.97      0.95       797\n",
      "          3       0.97      0.96      0.96       896\n",
      "\n",
      "avg / total       0.96      0.96      0.96      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.95       990\n",
      "          1       0.97      0.97      0.97      1067\n",
      "          2       0.93      0.97      0.95       800\n",
      "          3       0.97      0.97      0.97       894\n",
      "\n",
      "avg / total       0.96      0.96      0.96      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.95       992\n",
      "          1       0.97      0.98      0.97      1063\n",
      "          2       0.93      0.97      0.95       803\n",
      "          3       0.97      0.97      0.97       893\n",
      "\n",
      "avg / total       0.96      0.96      0.96      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.96       992\n",
      "          1       0.97      0.98      0.97      1061\n",
      "          2       0.94      0.97      0.95       806\n",
      "          3       0.97      0.97      0.97       892\n",
      "\n",
      "avg / total       0.96      0.96      0.96      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.94      0.96       991\n",
      "          1       0.97      0.98      0.97      1062\n",
      "          2       0.94      0.97      0.96       808\n",
      "          3       0.97      0.97      0.97       890\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.94      0.96       988\n",
      "          1       0.97      0.98      0.98      1061\n",
      "          2       0.95      0.97      0.96       814\n",
      "          3       0.97      0.98      0.98       888\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.95      0.96       985\n",
      "          1       0.97      0.98      0.98      1060\n",
      "          2       0.95      0.97      0.96       817\n",
      "          3       0.98      0.98      0.98       889\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.95      0.96       983\n",
      "          1       0.97      0.98      0.98      1061\n",
      "          2       0.95      0.97      0.96       817\n",
      "          3       0.98      0.98      0.98       890\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3751\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.95      0.97       982\n",
      "          1       0.97      0.98      0.98      1061\n",
      "          2       0.95      0.97      0.96       820\n",
      "          3       0.98      0.98      0.98       888\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.95      0.97       984\n",
      "          1       0.97      0.98      0.98      1062\n",
      "          2       0.95      0.97      0.96       818\n",
      "          3       0.98      0.98      0.98       887\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.96      0.97       981\n",
      "          1       0.97      0.98      0.98      1062\n",
      "          2       0.96      0.97      0.96       821\n",
      "          3       0.98      0.98      0.98       887\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.96      0.97       978\n",
      "          1       0.98      0.98      0.98      1065\n",
      "          2       0.96      0.97      0.97       824\n",
      "          3       0.98      0.99      0.98       884\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.96      0.97       978\n",
      "          1       0.98      0.98      0.98      1064\n",
      "          2       0.96      0.98      0.97       825\n",
      "          3       0.98      0.99      0.98       884\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.96      0.98       976\n",
      "          1       0.98      0.98      0.98      1066\n",
      "          2       0.96      0.98      0.97       823\n",
      "          3       0.98      0.99      0.99       886\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98       978\n",
      "          1       0.98      0.98      0.98      1066\n",
      "          2       0.97      0.98      0.97       823\n",
      "          3       0.98      0.99      0.99       884\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98       977\n",
      "          1       0.98      0.98      0.98      1066\n",
      "          2       0.97      0.98      0.97       822\n",
      "          3       0.99      0.99      0.99       886\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98       976\n",
      "          1       0.98      0.98      0.98      1065\n",
      "          2       0.97      0.98      0.97       823\n",
      "          3       0.99      0.99      0.99       887\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98       975\n",
      "          1       0.98      0.98      0.98      1066\n",
      "          2       0.97      0.98      0.97       823\n",
      "          3       0.99      0.99      0.99       887\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98       975\n",
      "          1       0.98      0.99      0.98      1065\n",
      "          2       0.97      0.98      0.97       823\n",
      "          3       0.99      0.99      0.99       888\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98       973\n",
      "          1       0.98      0.99      0.98      1067\n",
      "          2       0.97      0.98      0.97       823\n",
      "          3       0.99      0.99      0.99       888\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98       972\n",
      "          1       0.99      0.98      0.98      1071\n",
      "          2       0.97      0.98      0.97       821\n",
      "          3       0.99      0.99      0.99       887\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98       970\n",
      "          1       0.99      0.98      0.98      1072\n",
      "          2       0.97      0.98      0.98       823\n",
      "          3       0.99      0.99      0.99       886\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98       967\n",
      "          1       0.99      0.98      0.99      1072\n",
      "          2       0.97      0.98      0.98       826\n",
      "          3       0.99      0.99      0.99       886\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98       967\n",
      "          1       0.99      0.98      0.99      1074\n",
      "          2       0.97      0.98      0.98       825\n",
      "          3       0.99      1.00      0.99       885\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       968\n",
      "          1       0.99      0.99      0.99      1073\n",
      "          2       0.97      0.98      0.98       825\n",
      "          3       0.99      1.00      0.99       885\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       969\n",
      "          1       0.99      0.99      0.99      1073\n",
      "          2       0.97      0.98      0.98       823\n",
      "          3       0.99      1.00      0.99       886\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       965\n",
      "          1       0.99      0.99      0.99      1074\n",
      "          2       0.97      0.99      0.98       824\n",
      "          3       0.99      1.00      0.99       888\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       964\n",
      "          1       0.99      0.99      0.99      1075\n",
      "          2       0.97      0.99      0.98       824\n",
      "          3       0.99      1.00      0.99       888\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       963\n",
      "          1       0.99      0.99      0.99      1074\n",
      "          2       0.97      0.99      0.98       825\n",
      "          3       0.99      1.00      0.99       889\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       963\n",
      "          1       0.99      0.99      0.99      1074\n",
      "          2       0.97      0.99      0.98       825\n",
      "          3       0.99      1.00      0.99       889\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       962\n",
      "          1       0.99      0.99      0.99      1073\n",
      "          2       0.98      0.99      0.98       827\n",
      "          3       0.99      1.00      0.99       889\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       963\n",
      "          1       0.99      0.99      0.99      1073\n",
      "          2       0.98      0.99      0.98       827\n",
      "          3       0.99      1.00      0.99       888\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       963\n",
      "          1       0.99      0.99      0.99      1073\n",
      "          2       0.98      0.99      0.98       827\n",
      "          3       0.99      1.00      0.99       888\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3751\n",
      "\n",
      "saved synapses to: weights.json\n",
      " Shape of X is  (7587, 938)\n",
      " Shape of Y is  (4, 938)\n",
      " Shape of m is  938\n",
      "################### TEST MODEL STATISTICS ######################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94       215\n",
      "          1       0.93      0.93      0.93       271\n",
      "          2       0.92      0.93      0.93       221\n",
      "          3       0.97      0.95      0.96       231\n",
      "\n",
      "avg / total       0.94      0.94      0.94       938\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAH0ZJREFUeJzt3Xt0nHd95/H3d2Y0I83oLo3li2RLdmxiJyQxKHa4hNygBMpJui0sBNgCC81ugZa2bLe024VCT08LPVugewJsYAMthxLCFkjKhqbZXEiAXCyTe5zEl9iWLNuSdb9LI333jxkbWZatiTzSaJ75vM6ZM/M8z08z3wfGn/nl9/ye5zF3R0REgiWU7wJERCT3FO4iIgGkcBcRCSCFu4hIACncRUQCSOEuIhJACncRkQBSuIuIBJDCXUQkgCL5+uD6+npvbm7O18eLiBSk3bt3n3D35ELt8hbuzc3NtLW15evjRUQKkpkdyqadhmVERAJI4S4iEkAKdxGRAFK4i4gEkMJdRCSAFO4iIgG0YLib2W1m1mVmzy7Q7nIzmzazd+auPBERWYxseu7fAq4/VwMzCwOfB+7JQU3n9OKxIb7wry8wMDa11B8lIlKwFgx3d38I6F2g2e8B/wx05aKoczncO8pXHtzPoZ6Rpf4oEZGCdd5j7ma2Dvh3wNfOv5yFNdaUAdDRN7YcHyciUpBycUD1S8CfuPv0Qg3N7GYzazOztu7u7kV92Mlwb+8dXdTfi4gUg1xcW6YVuN3MAOqBt5tZyt1/NLehu98K3ArQ2trqi/mwitISquMl6rmLiJzDeYe7u7ecfG1m3wJ+PF+w51JjTRkdfeq5i4iczYLhbmbfBa4G6s2sA/gMUALg7ssyzj5XY3Wcfd3D+fhoEZGCsGC4u/tN2b6Zu3/wvKrJUlNtGQ++1IW7kxkOEhGRWQryDNXGmjjjUzP0jEzmuxQRkRWpQMNdM2ZERM6lIMO9qTYOaK67iMjZFGS4r6vWiUwiIudSkOGeiEWoTURp13RIEZF5FWS4AzTVlKnnLiJyFgUb7o01cZ3IJCJyFgUc7ume+8zMoq5iICISaIUb7rVxJlMznBieyHcpIiIrTuGG+8m57hp3FxE5Q8GGe9Op67pr3F1EZK6CDffGGp3IJCJyNgUb7qUlYerLY7oEgYjIPAo23OFXM2ZEROR0BR3uTbWa6y4iMp+CDvfGmjKO9I8xrbnuIiKnKfhwn5p2uobG812KiMiKUtDh3qQZMyIi8yrocNdNO0RE5lfQ4b5W13UXEZlXQYd7aUmYVRUxzZgREZmjoMMd0tMh23vVcxcRmW3BcDez28ysy8yePcv295nZ05nHL8zs0tyXeXaNNWV09KvnLiIyWzY9928B159j+8vAVe5+CfCXwK05qCtrjTVlHO0fJzU9s5wfKyKyoi0Y7u7+ENB7ju2/cPe+zOKjQGOOastKU02c1IxzbFBz3UVETsr1mPuHgZ+cbaOZ3WxmbWbW1t3dnZMP1NUhRUTOlLNwN7NrSIf7n5ytjbvf6u6t7t6aTCZz8rmNNZoOKSIyVyQXb2JmlwDfAN7m7j25eM9sra0uw0wnMomIzHbePXczWw/8APgP7v7S+Zf0ykQjIVZXlqrnLiIyy4I9dzP7LnA1UG9mHcBngBIAd/8a8GmgDviKmQGk3L11qQqeT/q67uq5i4ictGC4u/tNC2z/CPCRnFW0CE01cR57+awTekREik7Bn6EKmbnuA2NMaa67iAgQmHCPM+NwbEBz3UVEICjhXqtL/4qIzBaIcNdNO0RETheIcF9dVUrI0IwZEZGMQIR7STjEmqoy2tVzFxEBAhLuoLnuIiKzBSjc4xpzFxHJCEy4N9WWcWxwnInUdL5LERHJu8CEe2NNHHc42q+57iIiAQp3XfpXROSkwIR7U216rvuh3pE8VyIikn+BCfc1laWUloR4uVvhLiISmHAPhYzmugQHTijcRUQCE+4Am5LlvKxwFxEJVri31Cc43DvKZEqX/hWR4haocN+YTDA94xzW1SFFpMgFLNzLATQ0IyJFL1Dh3lKfAOBA93CeKxERya9AhXtVWQn15VEOaDqkiBS5QIU7wMb6cg6cUM9dRIpb8MI9mdCYu4gUvQXD3cxuM7MuM3v2LNvNzP7ezPaZ2dNm9prcl5m9lvoEJ4YnGRibymcZIiJ5lU3P/VvA9efY/jZgc+ZxM/DV8y9r8U7OmNFBVREpZguGu7s/BPSeo8mNwD962qNAtZmtyVWBr9TGZHrGjIZmRKSY5WLMfR3QPmu5I7PuDGZ2s5m1mVlbd3d3Dj76TE01ccIh04wZESlquQh3m2edz9fQ3W9191Z3b00mkzn46DNFIyHW18Y1Y0ZEilouwr0DaJq13Ah05uB9F21jfUI9dxEparkI97uA387MmrkCGHD3ozl430VrqU9Ph5yZmfc/IEREAi+yUAMz+y5wNVBvZh3AZ4ASAHf/GnA38HZgHzAKfGipis3WxmQ5E6kZOgfGaKyJ57scEZFlt2C4u/tNC2x34GM5qygHTs6YOdA9onAXkaIUuDNUQdMhRUQCGe7J8hjlsYhOZBKRohXIcDczNiZ1P1URKV6BDHfQdEgRKW6BDfeW+nKO9I8xPjWd71JERJZdYMNdB1VFpJgFPtw1NCMixSiw4X7yfqov6xozIlKEAhvu8WiENVWl6rmLSFEKbLhDemhmv8bcRaQIBTvc68s50D1M+goJIiLFI9Dh3lKfYGg8Rc/IZL5LERFZVoEOd82YEZFiFehw36SbZYtIkQp0uK+tLiMaCelEJhEpOoEO93DIaK6Ls1/DMiJSZAId7pCZMaMTmUSkyAQ/3JMJDveMMjU9k+9SRESWTeDDvaU+QWrG6egby3cpIiLLJvDhvlEzZkSkCAU+3DdprruIFKGswt3MrjezF81sn5l9ap7t683sATN7wsyeNrO3577UxamOR6lLRHnx+FC+SxERWTYLhruZhYFbgLcB24CbzGzbnGZ/Dtzh7tuB9wBfyXWh52P7+mp2H+rLdxkiIssmm577DmCfux9w90ngduDGOW0cqMy8rgI6c1fi+dvRUsvLJ0boGhrPdykiIssim3BfB7TPWu7IrJvtL4D3m1kHcDfwezmpLkcub64FoO2geu8iUhyyCXebZ93ca+jeBHzL3RuBtwPfNrMz3tvMbjazNjNr6+7ufuXVLtLF66ooKwnz+Mu9y/aZIiL5lE24dwBNs5YbOXPY5cPAHQDu/ghQCtTPfSN3v9XdW929NZlMLq7iRSgJh9i+vlrhLiJFI5tw3wVsNrMWM4uSPmB615w2h4HrAMxsK+lwX76ueRYub65lz7FBBsen8l2KiMiSWzDc3T0FfBy4B9hDelbMc2b2OTO7IdPsk8DvmNlTwHeBD/oKu/3RjpZa3NGsGREpCpFsGrn73aQPlM5e9+lZr58H3pDb0nJr+/pqIiFj18u9XPOqVfkuR0RkSQX+DNWT4tEIF6+rYtdBjbuLSPAVTbhDemjmqfYBxqem812KiMiSKqpwv7y5lsnpGZ5q7893KSIiS6qowr11Qw2AhmZEJPCKKtxrElG2NJTzuM5UFZGAK6pwh/TQzC8P9TE9s6JmaoqI5FTRhfuOllqGJ1LsOTqY71JERJZMUYY7sKhLEfSPTvL5f32B0clUrssSEcmpogv3NVVlNNaULSrcv/rT/Xz1wf3ct6drCSoTEcmdogt3gB3Ntew62MsruULCwNgU33n0MACPHuhZqtJERHKiKMP98pZaekYmOXAi+/uqfvuRgwxPpGipT/CYri4pIitccYZ75uYdu7IM6bHJaW77+UGueVWSd1/exL6uYbqHJpayRBGR81KU4b4pmaAuEeXxLE9m+t6uw/SOTPLRay5g53kckBURWS5FGe5mxuWZcfeFTE3P8PWHX6Z1Qw2XN9dy8boqEtGwxt1FZEUrynCH9Lh7e+8YRwfGztnuzic7OdI/xkev2QSk7+r02uZaHntZ4S4iK1fRhvuO5oWHV2ZmnK/9dD8Xrq447RrwO1tqeen4MD3DGncXkZWpaMN965oKEtHwOYdm7t1znH1dw/zu1Zsw+9V9wq/YWAdo3F1EVq6iDfdIOMTrNtVxR1sHX7z3pTOu8e7ufOXB/ayvjfPrr15z2rZLGqsoKwlrSqSIrFhFG+4Af/2bl3D9Rav58n17eeuXHuKBF3915ukj+3t4qr2f/3TVRiLh0/9nKgmHaG2u0UFVEVmxijrckxUx/v6m7XznIzsJh4wPfXMX//nbu+nsH+MrD+4nWRHjt17TOO/f7myp5YVjQ/SNTC5z1SIiCyvqcD/pDRfU85NPXMkfv/VVPPhSF9f+jwf52b4TfPiNLZSWhOf9m5Pj7hqaEZGVSOGeEYuE+dg1F3DvH17FlZuTrK+N876d68/a/pLGakpLQpoSKSIrUiSbRmZ2PfBlIAx8w93/Zp42/x74C8CBp9z9vTmsc9k01cb5+m+3LtguGgnx2g01PHpAPXcRWXkW7LmbWRi4BXgbsA24ycy2zWmzGfhT4A3ufhHwB0tQ64qzs6WOF44NMjA6le9SREROk82wzA5gn7sfcPdJ4Hbgxjltfge4xd37ANy9KC54fsXGOtzJ+ho1IiLLJZtwXwe0z1ruyKybbQuwxcx+bmaPZoZxAu/SpipikZCmRIrIipPNmLvNs27uXS4iwGbgaqAReNjMLnb3/tPeyOxm4GaA9evPfrCyUMQiYbavr9ZBVRFZcbLpuXcATbOWG4HOedrc6e5T7v4y8CLpsD+Nu9/q7q3u3ppMJhdb84pyxcY6nuscZGBM4+4isnJkE+67gM1m1mJmUeA9wF1z2vwIuAbAzOpJD9McyGWhK9XOlvS4e5vG3UVkBVkw3N09BXwcuAfYA9zh7s+Z2efM7IZMs3uAHjN7HngA+GN3L4qxiu3rq4lq3F1EVpis5rm7+93A3XPWfXrWawf+KPMoKqUlYS5rqtaZqiKyougM1Ry4YmMdzx4ZYGhc4+4isjIo3HPgipZaZhzaDvbluxQREUDhnhOv2VBDLBLi4b0n8l2KiAigcM+J0pIwr99Ux30vHCd9+EFEJL8U7jly7dYGDvWMsr97JN+liIgo3HPl2gvTN9C+/4Xjea5EREThnjPrqsvYuqaS+/YUxTXTRGSFU7jn0HUXrqLtUJ8uASwieadwz6Frt65iesb56d7ufJciIkVO4Z5DlzZWU5eIcv8ejbuLSH4p3HMoHDKuftUqHnypm9T0TL7LEZEipnDPseu2rqJ/dIon2vsXbiwiskQU7jl25eZ6IiHTrBkRySuFe45VlJawc2Ot5ruLSF4p3JfAtRc28NLxYdp7R/NdiogUKYX7Erju1NmqGpoRkfxQuC+B5voEG5MJ7lO4i0ieKNyXyHUXruLR/T2MTKTyXYqIFCGF+xK59sIGJqdn+Nk+XeNdRJafwn2JtDbXUFEa4X5NiRSRPFC4L5GScIirtiS5/8UuZmZ0Aw8RWV4K9yV03dZVdA9N8GznQL5LEZEik1W4m9n1Zvaime0zs0+do907zczNrDV3JRauq7asImTwb8/phCYRWV4LhruZhYFbgLcB24CbzGzbPO0qgN8HHst1kYWqNhHl2gsb+PrDB3jh2GC+yxGRIpJNz30HsM/dD7j7JHA7cOM87f4S+AIwnsP6Ct5f/+arqSwr4WPf+aWmRYrIsskm3NcB7bOWOzLrTjGz7UCTu/84h7UFQrIixpfffRkHTozw3+98Nt/liEiRyCbcbZ51p6Z/mFkI+CLwyQXfyOxmM2szs7bu7uK5W9HrL6jn967dzA9+eYTvt7Uv/AciIucpm3DvAJpmLTcCnbOWK4CLgQfN7CBwBXDXfAdV3f1Wd29199ZkMrn4qgvQJ67bzBUba/n0nc+x9/hQvssRkYDLJtx3AZvNrMXMosB7gLtObnT3AXevd/dmd28GHgVucPe2Jam4QIVDxpffs514NMzH/umXjE1O57skEQmwBcPd3VPAx4F7gD3AHe7+nJl9zsxuWOoCg6ShspQvvvsy9nYN85m7NP4uIksnkk0jd78buHvOuk+fpe3V519WcL1pS5KPXr2JWx7Yz86WOn7rtY35LklEAkhnqObBH755Czuaa/nk95/iz374DAOjU/kuSUQCRuGeB5FwiG9+6HI+8sYWbn/8MNf93U+566lO3HUNGhHJDYV7niRiEf78Hdu46+NvZG11Kb//3Sf4wDd3cbhHt+YTkfOncM+zi9dV8cOPvoHP3nARvzzUx1u++FP+5317GRzXUI2ILJ7layigtbXV29o0W3K2YwPjfPZfnuMnzx6jPBbhXa2NfPD1zWyoS+S7NBFZIcxst7sveHFGhfsK9HRHP9/8+UH+5alOpt1589YGPvzGFna21GI23wnDIlIsFO4BcHxwnG8/cojvPHaIvtEptq2p5F2tjbzjkrUkK2L5Lk9E8kDhHiDjU9P86Ikj/OMjh3j+6CDhkPHGC+r5je1r+bVtq0nEsjpdQUQCQOEeUC8dH+JHTxzhzic7OdI/RllJmLdsa+Dtr17Dm7bUE48q6EWCTOEecDMzzu7DffzwiSP836ePMjA2RSwS4srN9bxlWwPXbW2gvlxDNyJBo3AvIlPTMzz+ci/3Pn+ce58/zpH+MczgNetruG7rKt60Ocm2NZWEQjoYK1LoFO5Fyt15/ujgqaB/rjN9e7/68ihXbk7ypi31XLk5qV69SIFSuAsAXUPjPPzSCR7a283De0/QOzIJwLY1lbxuUx1XbKxjR0stVWUlea5URLKhcJczzMw4z3UO8tDebn629wS7D/cxmZrBDC5aW8kVLemwb22uoToezXe5IjIPhbssaHxqmifb+3n0QA+P7O/hicP9TE7PAHDBqnIub67htRtqad1Qw4a6uE6gElkBFO7yio1PTfPE4X52H+ql7VAfuw/1MTSeAqC+PMZlTdVsX1/NpY3VXNJURWWphnJEllu24a5J0XJKaUmY122q43Wb6oD0MM7ermHaDvWy+2AfT3b08//2HAfADDYly7m0sZpLm6q4eF0VW1dXUhYN53MXRCRDPXd5RQZGp3iqo5+n2vt5MvPoyRykDYeMC5LlXLSuklevq+KitVVcuKZCPXyRHNKwjCwLd+fowDjPHBng2czjmSODnBieONWmqbaMrasr2bqmkm1rK9m2ppJ11WWady+yCBqWkWVhZqytLmNtdRlvvWg1kA7844MTPNc5wJ6jg+w5OsSeo4Pcu+c4J/sS8WiYzQ0VvKqhnC0NFVy4upItDeUkK2I6cCuSAwp3yTkzY3VVKaurSrlua8Op9aOTKV48NsQLx4Z48dgQLx0f4v4XurijreNUm8rSCJsbKrggWc7mhnIuWJV+rK1ST1/klVC4y7KJRyNsX1/D9vU1p60/MTzBS8eGePH4EPu6htnbNcy9e47zvbb2U21KS0K01JezKZlgYzL9vClZTnN9gnJdFVPkDFn9qzCz64EvA2HgG+7+N3O2/xHwESAFdAP/0d0P5bhWCaj68hj1F8R4/QX1p63vGZ44FfYHukc4cGKYpzsGuPuZo8zMOlSUrIjRXBenuS5Bc32ClvoEG+ribKhT8EvxWvCbb2Zh4BbgLUAHsMvM7nL352c1ewJodfdRM/td4AvAu5eiYCkedeUx6spj7NxYd9r6idQ0h3pGOdA9zIETIxw8McLBE6M8+FI33bs7Tn+PRJT1dXE21MZZX5dgQ22cpto4TbVlNFSUaqhHAiubbs0OYJ+7HwAws9uBG4FT4e7uD8xq/yjw/lwWKTJbLBJmS0MFWxoqztg2PJHi4IkRDvWMcqh3hMM9oxzqGWXXwT7ufKqT2ZPDouEQ62rKaKwpo6k2TmNNGeuqyzLPcVZVxBT+UrCyCfd1QPus5Q5g5znafxj4yXwbzOxm4GaA9evXZ1miSPbKYxEuXpc+qWquidQ0R/rGaO8bo713lPa+UTp6x2jvG+XZZ47SNzp1WvtoOMSa6lLWVqVnA62rLmVN9azXVWW6C5asWNl8M+frusw7Od7M3g+0AlfNt93dbwVuhfQ89yxrFMmJWCTMxmQ5G5Pl824fmUjR2T9GR98YHf1jHOkb40j/GJ39Y/xi/wmOD46fNtYPUFEaYU1VKaurylhTmZ4htKaqlIaqUlZXph/V8RJN75Rll024dwBNs5Ybgc65jczszcB/A65y94m520VWukQsPQ1z8zzDPQCp6RmOD03QmQn8owPjHM08HxscZ8/R9Mlbc88LjEZCNFTGWF1ZyqrKUhoqSllVGaOhMsaqilIaKmMkK0qpLI3oR0ByJptw3wVsNrMW4AjwHuC9sxuY2XbgfwHXu3tXzqsUWQEi4RDrqtPj8mczmZqha2ic44PjHBuY4Nhg+nV6eZw9nYM8ONjFyOT0GX8bjYRIlsdYVRkjWR4jWZF+1JenH8mK9Pr6iqjulSsLWvAb4u4pM/s4cA/pqZC3uftzZvY5oM3d7wL+FigHvp/peRx29xuWsG6RFSkaCdFYE6exJn7OdsMTKboGxzk+OEHX0DjdQxN0D03QlXlOHwTuPeM4wEmJaDgzmyiaCf/0c10iSm3mua48Sm0iSm08SiQcWordlRVM15YRWcGmpmfoGZ7kxHA69Lszzz3Dk/SMTHBieCKzfZLekYkzjgmcVB0vORX0NYkodYlZz/H0j0BNIkpNvISaRJSKmIaIVipdW0YkAErCoVOXcljI9IzTPzpJ78jJsE//APRkXp98tPeO8mR7P30jk6TO8msQCRnV8XTYV8dLTr2uiUepjkfT68pKfvU6XkJ1WZTSkpB+FFYIhbtIQIRDdurEr80NC7d3dwbHU6d+EPpGJ+kdmaJvZJLe0Un6RyfpG5mifyz9g/B0xyR9o1NMpmbO+p7RcIiqTPBXlaVDvzLzuqqshMrSX72uiqeXK8siVJWVUFYS1g9DDincRYqUmZ0K2g11iaz+xt0Zn5qhf2yS/tEp+kYnGRidon8s83psisGxKfpHpxgYm6Kzf5w9R4cYHJtiaCJ1zveOhIzKshIqSyOZ53TwV8Qyz6XpbRWlJVTMeq48tRzRsYVZFO4ikjUzoywapixaxpqqs88amk9qeoah8RSD4+ngT/8QpJcHTy6PTzEwlmIos+744Hhme4qxqTNnGM1VVhKmPBP0FaUlVMTSr8tjkfT6zHN5rIRELJzZlnkdK6G8NEIiFiYWKfw7iincRWRZRMKh9EHbRHRRfz+V+XEYGp869SORXk4xODbF8ER62/BEisHM+qHxKbqGxhnOLA9Pps44D2E+JWEjEcv8KMQiJDKP8liYRPTkcjj9fHI5GiaeaROPptfHM+3zcSxC4S4iBaEkHErP+FnkjwOk7ws8OjXN0PgUIxMphiemGR5PMTyReYxPMTI5zfBEKrM9xfB4ipHJVGaYaYzRzPqRyWmmzzY9aQ4zMj8C6bB/7871fOTKjYvej2wo3EWkaIRCdqo3fr7cnYnUDCMTKUYzPwijkylGJqZP/TCMTU0zMjF9av3oZPpHob48loO9OTeFu4jIIpgZpSVhSkvC1C3cfNnp0LKISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIiAaRwFxEJoLzdrMPMuoFDi/zzeuBEDsspJMW679rv4qL9PrsN7p5c6I3yFu7nw8zasrkTSRAV675rv4uL9vv8aVhGRCSAFO4iIgFUqOF+a74LyKNi3Xftd3HRfp+nghxzFxGRcyvUnruIiJxDwYW7mV1vZi+a2T4z+1S+61kqZnabmXWZ2bOz1tWa2b1mtjfzXJPPGpeCmTWZ2QNmtsfMnjOzT2TWB3rfzazUzB43s6cy+/3ZzPoWM3sss9/fM7PF34ZoBTOzsJk9YWY/ziwHfr/N7KCZPWNmT5pZW2Zdzr7nBRXuZhYGbgHeBmwDbjKzbfmtasl8C7h+zrpPAfe5+2bgvsxy0KSAT7r7VuAK4GOZ/4+Dvu8TwLXufilwGXC9mV0BfB74Yma/+4AP57HGpfQJYM+s5WLZ72vc/bJZ0x9z9j0vqHAHdgD73P2Au08CtwM35rmmJeHuDwG9c1bfCPxD5vU/AL+xrEUtA3c/6u6/zLweIv0Pfh0B33dPG84slmQeDlwL/J/M+sDtN4CZNQK/Dnwjs2wUwX6fRc6+54UW7uuA9lnLHZl1xaLB3Y9COgSBVXmuZ0mZWTOwHXiMItj3zNDEk0AXcC+wH+h391SmSVC/718C/iswk1muozj224F/M7PdZnZzZl3OvueFdg9Vm2edpvsEkJmVA/8M/IG7D6Y7c8Hm7tPAZWZWDfwQ2Dpfs+WtammZ2TuALnffbWZXn1w9T9NA7XfGG9y908xWAfea2Qu5fPNC67l3AE2zlhuBzjzVkg/HzWwNQOa5K8/1LAkzKyEd7N9x9x9kVhfFvgO4ez/wIOljDtVmdrITFsTv+xuAG8zsIOlh1mtJ9+SDvt+4e2fmuYv0j/kOcvg9L7Rw3wVszhxJjwLvAe7Kc03L6S7gA5nXHwDuzGMtSyIz3vq/gT3u/nezNgV6380smemxY2ZlwJtJH294AHhnplng9tvd/9TdG929mfS/5/vd/X0EfL/NLGFmFSdfA78GPEsOv+cFdxKTmb2d9C97GLjN3f8qzyUtCTP7LnA16avEHQc+A/wIuANYDxwG3uXucw+6FjQzeyPwMPAMvxqD/TPS4+6B3Xczu4T0AbQw6U7XHe7+OTPbSLpHWws8Abzf3SfyV+nSyQzL/Bd3f0fQ9zuzfz/MLEaAf3L3vzKzOnL0PS+4cBcRkYUV2rCMiIhkQeEuIhJACncRkQBSuIuIBJDCXUQkgBTuIiIBpHAXEQkghbuISAD9f22EOinC4XxNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Here I am loading the dataset from stored folder. The training data is stored as text file and each tweet is accompanied\n",
    "# by the magnitude of its sentiment (0 to 1). I had to go through the tweets myself and observed that a threshold of 0.5 is \n",
    "# good enough to classify a tweet according to its sentiment. Tweets with lesser threshold were not definitive to be trained as per their mentioned classification  \n",
    "# I only read those tweets that have a dominant classification factor i.e. above 0.5\n",
    "# Here i am setting each tweet's threshold magnitude accordingly\n",
    "def load_training_data(sentiment):\n",
    "    data = open(\"./datasets/\"+sentiment+\"_training_set.txt\",encoding=\"utf8\")\n",
    "    if sentiment == \"anger\":        \n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"fear\":\n",
    "        threshold = 0.6\n",
    "    elif sentiment == \"sadness\":\n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"joy\":\n",
    "        threshold = 0.5\n",
    "    else:\n",
    "        pass\n",
    "    return data,threshold\n",
    "\n",
    "\n",
    "def load_test_data(sentiment):\n",
    "    data = open(\"./datasets/\"+sentiment+\"_test_set.txt\",encoding=\"utf8\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# In this method, I am cleaning the tweet data removing punctuations and then tokenizing the words in tweet removing name tags\n",
    "# and appending them to training set\n",
    "def clean_data(training_data,threshold):\n",
    "    training_set = []\n",
    "    for line in training_data:\n",
    "        line = line.strip().lower()\n",
    "        if line.split()[-1] == \"none\":\n",
    "            line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "            punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "            result = line.translate(punct)\n",
    "            tokened_sentence = nltk.word_tokenize(result)\n",
    "            sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "            label = tokened_sentence[-2]\n",
    "            training_set.append((sentence,label))\n",
    "        else:\n",
    "            intensity = float(line.split()[-1])        \n",
    "            if (intensity>=threshold):\n",
    "                line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "                punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "                result = line.translate(punct)\n",
    "                tokened_sentence = nltk.word_tokenize(result)\n",
    "                sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "                label = tokened_sentence[-1]\n",
    "                training_set.append((sentence,label))\n",
    "    return training_set\n",
    "    \n",
    "# This method collects all the unique words that are contained in the entire tweet dataset, finds their stem and \n",
    "# encodes each sentence according to the bag of words appending it to training set\n",
    "def bag_of_words(all_data):\n",
    "    training_set = []\n",
    "    all_words = []\n",
    "    for each_list in all_data:\n",
    "        for words in each_list[0]:\n",
    "            word = stemmer.stem(words)\n",
    "            all_words.append(word)\n",
    "    all_words = list(set(all_words))\n",
    "    \n",
    "    for each_sentence in all_data:  \n",
    "        bag = [0]*len(all_words)\n",
    "        training_set.append(encode_sentence(all_words,each_sentence[0],bag))\n",
    "    return training_set,all_words\n",
    "\n",
    "# Here we encode each tweet's words according to the words it contained from the bag of words which is based on all words in all tweets\n",
    "def encode_sentence(all_words,sentence, bag):\n",
    "    for s in sentence:        \n",
    "        stemmed_word = stemmer.stem(s)\n",
    "        for i,word in enumerate(all_words):\n",
    "            if stemmed_word == word:\n",
    "                bag[i] = 1\n",
    "    return bag\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    bag = [] \n",
    "    all_data = []\n",
    "    all_test_data = []\n",
    "    labels = []\n",
    "    classes = []\n",
    "    labels = []\n",
    "    test_labels = []\n",
    "    words=[]\n",
    "    test_words = []\n",
    "        \n",
    "    ######### Here we read the whole training data for each class and the threshold we will use for its classification\n",
    "    anger_training_data,threshold = load_training_data(\"anger\")\n",
    "    anger_training_set = clean_data(anger_training_data,threshold)\n",
    "    print(anger_training_set[0])\n",
    "    \n",
    "    fear_training_data,threshold = load_training_data(\"fear\")\n",
    "    fear_training_set = clean_data(fear_training_data,threshold)\n",
    "    \n",
    "    sadness_training_data,threshold = load_training_data(\"sadness\")\n",
    "    sadness_training_set = clean_data(sadness_training_data,threshold)\n",
    "    \n",
    "    joy_training_data,threshold = load_training_data(\"joy\")\n",
    "    joy_training_set = clean_data(joy_training_data,threshold)\n",
    "    \n",
    "    \n",
    "    ######### Here we read the whole test data for each class and the threshold we will use for its classification\n",
    "    anger_test_data = load_test_data(\"anger\")\n",
    "    anger_test_set = clean_data(anger_test_data,threshold)\n",
    "    #print(anger_test_set[0])\n",
    "    print(len(anger_test_set))\n",
    "    \n",
    "    fear_test_data = load_test_data(\"fear\")\n",
    "    fear_test_set = clean_data(fear_test_data,threshold)\n",
    "   # print(fear_test_set[0])\n",
    "    print(len(fear_test_set))\n",
    "    \n",
    "    sadness_test_data = load_test_data(\"sadness\")\n",
    "    sadness_test_set = clean_data(sadness_test_data,threshold)\n",
    "  #  print(sadness_test_set[0])\n",
    "    print(len(sadness_test_set))\n",
    "    \n",
    "    joy_test_data = load_test_data(\"joy\")\n",
    "    joy_test_set = clean_data(joy_test_data,threshold)\n",
    "  #  print(joy_test_set[0])\n",
    "    print(len(joy_test_set))\n",
    "    ###### In every training set above we have a nested list whose first element is sentence and 2nd element its respective label ######\n",
    "    \n",
    "#    print(anger_training_set[0][0],anger_training_set[0][1])\n",
    "#    print(joy_training_set[0][0],joy_training_set[0][1])\n",
    "\n",
    "    \n",
    "    ###### Here we combine all training sets in one list ######\n",
    "    all_data.extend(anger_training_set)\n",
    "    all_data.extend(fear_training_set)\n",
    "    all_data.extend(sadness_training_set)\n",
    "    all_data.extend(joy_training_set)\n",
    "    \n",
    "    all_data.extend(anger_test_set)\n",
    "    all_data.extend(fear_test_set)\n",
    "    all_data.extend(sadness_test_set)\n",
    "    all_data.extend(joy_test_set)\n",
    "    \n",
    "    ###### Here we simply make a classification label list encoding our 4 classes as follows\n",
    "    \n",
    "    \n",
    "    for i,j in all_data:\n",
    "        if j == \"anger\":            \n",
    "            labels.append([1,0,0,0])\n",
    "        elif j == \"fear\":            \n",
    "            labels.append([0,1,0,0])\n",
    "        elif j == \"sadness\":            \n",
    "            labels.append([0,0,1,0])\n",
    "        elif j == \"joy\":            \n",
    "            labels.append([0,0,0,1])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    print(len(labels))\n",
    "    print(len(test_labels))\n",
    "    classes = [\"anger\",\"fear\",\"sadness\",\"joy\"]\n",
    "    print(classes)\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    \n",
    "    # Here we will have the whole training set and the all the words contained in whole training set\n",
    "    training_set,words = bag_of_words(all_data)\n",
    "    \n",
    "    # We convert our training,test set and training, test labels in a numpy array as it is required for calculations in neural net\n",
    "    dataset = np.array(training_set)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # It is important to shuffle dataset so your classifier does not attempt to memorize training set, this functions shuffles data and labels.\n",
    "    shuffling_function = np.random.permutation(dataset.shape[0])\n",
    "    shuffled_dataset, shuffled_labels = np.zeros((dataset.shape)),np.zeros((dataset.shape))\n",
    "    shuffled_dataset,shuffled_labels = dataset[shuffling_function],labels[shuffling_function]\n",
    "    \n",
    "    \n",
    "    split = int(len(shuffled_dataset)*0.8)\n",
    "    training_data = shuffled_dataset[:split]\n",
    "    training_labels = shuffled_labels[:split]\n",
    "    test_data = shuffled_dataset[split:]\n",
    "    test_labels = shuffled_labels[split:]\n",
    "    print(training_data.shape)\n",
    "    print(training_labels.shape)    \n",
    "    print(test_data.shape)\n",
    "    print(test_labels.shape)\n",
    "    \n",
    "        \n",
    "    ############# HERE WE HAVE A SHUFFLED DATASET WITH RESPECTIVE LABELS NOW WE HAVE TO TRAIN THIS DATA BOTH NUMPY ARRAYS ############\n",
    "    Train_model(training_data,training_labels,words,classes)\n",
    "    Test_model(test_data,test_labels,words,classes)\n",
    "\n",
    "# Method for calculating sigmoid\n",
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))\n",
    "    \n",
    "# Method for calculating relu\n",
    "def relu(z):\n",
    "    A = np.array(z,copy=True)\n",
    "    A[z<0]=0\n",
    "    assert A.shape == z.shape\n",
    "    return A\n",
    "    \n",
    "# Method for calculating softmax\n",
    "def softmax(x):\n",
    "    num = np.exp(x-np.amax(x,axis=0,keepdims=True))    \n",
    "    return num/np.sum(num,axis=0,keepdims=True)\n",
    "\n",
    "# Method for calculating forward propagation\n",
    "def forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2):\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "\n",
    "# Method for calculating relu activation's derivative\n",
    "def relu_backward(da,dz):\n",
    "    da1 = np.array(da,copy=True)\n",
    "    da1[dz<0]=0\n",
    "    assert da1.shape == dz.shape\n",
    "    return da1\n",
    "\n",
    "# Method for calculating linear part of backward propagation\n",
    "def linear_backward(dz,a,m,w,b):\n",
    "    dw = (1/m)*np.dot(dz,a.T)\n",
    "    db = (1/m)*np.sum(dz,axis=1,keepdims=True)\n",
    "    da = np.dot(w.T,dz)\n",
    "    assert (dw.shape==w.shape)\n",
    "    assert (da.shape==a.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return da,dw,db \n",
    "\n",
    "# Method for calculating loss function\n",
    "def calculate_loss(Y,Yhat,m):\n",
    "    loss = (-1/m)*np.sum(np.multiply(Y,np.log(Yhat)))\n",
    "    return loss\n",
    "\n",
    "# Method for back propagation\n",
    "def back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m):\n",
    "    dZ2 = A2-Y\n",
    "    da1,dw2,db2 = linear_backward(dZ2,A1,m,W2,b2)\n",
    "    dZ1 = relu_backward(da1,Z1)\n",
    "    da0,dw1,db1 = linear_backward(dZ1,X,m,W1,b1)\n",
    "    W2 = W2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "\n",
    "# Method for training model\n",
    "def Test_model(test_data, test_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = test_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = test_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    \n",
    "    weights_file = 'weights.json' \n",
    "    with open(weights_file) as data_file: \n",
    "        weights = json.load(data_file) \n",
    "        W1 = np.asarray(weights['weight1']) \n",
    "        W2 = np.asarray(weights['weight2'])\n",
    "        b1 = np.asarray(weights['bias1']) \n",
    "        b2 = np.asarray(weights['bias2'])\n",
    "\n",
    "    print(\"################### TEST MODEL STATISTICS ######################\")\n",
    "    for i in range(1):\n",
    "        # input layer is our encoded sentence\n",
    "        l0 = X\n",
    "        # matrix multiplication of input and hidden layer\n",
    "        l1 = relu(np.dot(W1,l0)+b1)\n",
    "        # output layer\n",
    "        l2 = softmax(np.dot(W2,l1)+b2)\n",
    "        predictions = np.argmax(l2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "\n",
    "\n",
    "\n",
    "# Method for training model\n",
    "def Train_model(training_data, training_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = training_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = training_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    # Multiplying by 0.01 so that we get smaller weights .. dimensions 100x3787\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    print(\" Shape of W1 is \", W1.shape)\n",
    "    # Dimensions 100x1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    # Dimensions 1547 x 4\n",
    "    W2 = np.random.randn(n_y,n_h)\n",
    "    print(\" Shape of W2 is \", W2.shape)\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    print(\"################### TRAIN MODEL STATISTICS ######################\")\n",
    "    for i in range(0,iterations):\n",
    "        Z1,A1,Z2,A2 = forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2)\n",
    "        predictions = np.argmax(A2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "        Loss = calculate_loss(Y,A2,m)\n",
    "        W1,b1,W2,b2 = back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m)\n",
    "        all_losses.append(Loss)\n",
    "\n",
    "    # storing weights so that we can reuse them without having to retrain the neural network\n",
    "    weights = {'weight1': W1.tolist(), 'weight2': W2.tolist(), \n",
    "               'bias1':b1.tolist(), 'bias2':b2.tolist(),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    weights_file = \"weights.json\"\n",
    "\n",
    "    with open(weights_file, 'w') as outfile:\n",
    "        json.dump(weights, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", weights_file)\n",
    "    plt.plot(all_losses)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to kill everyone @Name1 #why? \n",
      " classification: [['anger', array([0.63564561])], ['fear', array([0.17131302])], ['joy', array([0.13189824])]] \n",
      "\n",
      "I am so happy @Name2 #yayyyy \n",
      " classification: [['joy', array([0.69211525])], ['anger', array([0.16699474])]] \n",
      "\n",
      "This depression will kill me someday .. i am dying @Name3 #killme \n",
      " classification: [['sadness', array([0.68720888])], ['fear', array([0.15805518])]] \n",
      "\n",
      "I am afraid terrorists might attack us @Name4 #isis \n",
      " classification: [['fear', array([0.95233197])]] \n",
      "\n",
      "What should I do when i am happy @Name5  \n",
      " classification: [['joy', array([0.3433974])], ['anger', array([0.26879196])], ['fear', array([0.23412605])], ['sadness', array([0.15368459])]] \n",
      "\n",
      "I want to be happy \n",
      " classification: [['joy', array([0.66720538])], ['anger', array([0.13739311])], ['fear', array([0.11710301])]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['joy', array([0.66720538])],\n",
       " ['anger', array([0.13739311])],\n",
       " ['fear', array([0.11710301])]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.1\n",
    "# load our calculated weight values\n",
    "weights_file = 'weights.json' \n",
    "with open(weights_file) as data_file: \n",
    "    weights = json.load(data_file) \n",
    "    W1 = np.asarray(weights['weight1']) \n",
    "    W2 = np.asarray(weights['weight2'])\n",
    "    b1 = np.asarray(weights['bias1']) \n",
    "    b2 = np.asarray(weights['bias2'])\n",
    "    all_words = weights['words']\n",
    "    classes = weights['classes']\n",
    "    \n",
    "def clean_sentence(verification_data):\n",
    "    line = verification_data\n",
    "    # Remove whitespace from line and lower case iter\n",
    "    line = line.strip().lower()\n",
    "    # Removing word with @ sign as we dont need name tags of twitter\n",
    "    line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "    # Remove punctuations and numbers from the line\n",
    "    punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "    result = line.translate(punct)\n",
    "    # Tokenize the whole tweet sentence\n",
    "    tokened_sentence = nltk.word_tokenize(result)\n",
    "    # We take the tweet sentence from tokened sentence\n",
    "    sentence = tokened_sentence[0:len(tokened_sentence)]\n",
    "    return sentence    \n",
    "\n",
    "def verify(sentence, show_details=False):\n",
    "    bag=[0]*len(all_words)\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    # This line returns the bag of words as 0 or 1 if words in sentence are found in all_words\n",
    "    x = encode_sentence(all_words,cleaned_sentence,bag)\n",
    "    x = np.array(x)\n",
    "    x = x.reshape(x.shape[0],1)\n",
    "    \n",
    "#    print(\"Shape of X is \", x.shape)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our encoded sentence\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = relu(np.dot(W1,l0)+b1)\n",
    "    # output layer\n",
    "    l2 = softmax(np.dot(W2,l1)+b2)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = verify(sentence, show_details)\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    print (\"%s \\n classification: %s \\n\" % (sentence, return_results))\n",
    "    return return_results\n",
    "\n",
    "classify(\"I want to kill everyone @Name1 #why?\")\n",
    "classify(\"I am so happy @Name2 #yayyyy\")\n",
    "classify(\"This depression will kill me someday .. i am dying @Name3 #killme\")\n",
    "classify(\"I am afraid terrorists might attack us @Name4 #isis\")\n",
    "classify(\"What should I do when i am happy @Name5 \")\n",
    "classify(\"I want to be happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "input_sentiment = input(\"Hi :) How are you feeling today ? \")\n",
    "#print(input_sentiment)\n",
    "#print(classify(input_sentiment)[0][0])\n",
    "sentiment = classify(input_sentiment)[0][0]\n",
    "print(sentiment)\n",
    "if sentiment == \"anger\" or sentiment == \"sadness\" or sentiment == \"fear\":\n",
    "    answer = input(\"Sorry to hear that .... would you like to hear a joke to lighten your mood ? Press Yes or No \")\n",
    "    if answer == \"N\" or answer == \"No\" or answer == \"no\" or answer == \"n\":\n",
    "        print(\"Have a nice day. Goodbye :) \")\n",
    "    else:\n",
    "        file = open('./datasets/jokes.txt','r')\n",
    "        while(1):\n",
    "            full_file = file.readline()\n",
    "            split_file = full_file.split('/')\n",
    "        #    print(split_file)\n",
    "            slashes = full_file.count('/')\n",
    "        #    print(slashes)\n",
    "            line_of_joke = []\n",
    "            for i in range(slashes):\n",
    "                k=0\n",
    "            #    print(split_file[i])\n",
    "                commas = split_file[i].count('\"')\n",
    "        #        print(commas)\n",
    "                length = int(commas/2)\n",
    "                if length == 0:\n",
    "                    line_of_joke.append(split_file[i])\n",
    "                else:\n",
    "                    for j in range(length):\n",
    "        #            print(\"Here\")\n",
    "                        line_of_joke.append(split_file[i].split('\"')[k]+split_file[i].split('\"')[k+1])\n",
    "                        if j==length-1:\n",
    "                            line_of_joke.append(split_file[i].split('\"')[k+2])\n",
    "                        k=k+2\n",
    "    #    break\n",
    "            for i in line_of_joke:\n",
    "                print(i)\n",
    "            user_input = input(\"Do you want another joke ? Write Yes or No\\t\")\n",
    "            if user_input == \"Y\":\n",
    "                clear_output()\n",
    "                pass\n",
    "            else:\n",
    "                clear_output()\n",
    "                break\n",
    "        #print(line_of_joke[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
